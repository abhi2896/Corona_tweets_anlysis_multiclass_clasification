# -*- coding: utf-8 -*-
"""NLP_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1559DWZLYVECn2TIXUEcpl5_X0mT_RVIs
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv("Corona_NLP_train.csv",  encoding='latin-1')

df.sample(5)

df.drop(columns = ["UserName",	"ScreenName",	"Location",	"TweetAt"] , inplace = True)

df.sample(5)

df.iloc[39164]

"""
# 1. Data cleaning
# 2. EDA
# 3. Text Preprocessing
# 4. Model building
# 5. Evaluation
# 6. Improvement
# 7. Website
# 8. Deploy
```
# This is formatted as code
```



"""

#Data cleaning
df.isnull().sum()

df.duplicated().sum()

df.info()

df.Sentiment.isnull().sum()

df.Sentiment.value_counts()

df["Sentiment"] = df["Sentiment"].map({"Extremely Negative" : 0 , "Negative" : 1 , "Neutral" : 2 , "Positive" : 3 , "Extremely Positive" : 4})

#from sklearn.preprocessing import LabelEncoder
#encoder = LabelEncoder()
#df['Sentiment'] = encoder.fit_transform(df['Sentiment'])
# ex. neg = 0 , ex. positive = 1 , neg = 2 , neutral = 3 , positive = 4

df.Sentiment.sample(5)

df.shape

#2.EDA
df.Sentiment.value_counts() # data is a bit imbalance

import matplotlib.pyplot as plt
plt.pie(df['Sentiment'].value_counts(),autopct="%0.2f")
plt.show()

import nltk
nltk.download('punkt')

#creating a new feature
# length of tweet
df['num_characters'] = df['OriginalTweet'].apply(len)
#char in tweet
df['num_words'] = df['OriginalTweet'].apply(lambda x:len(nltk.word_tokenize(x)))
#number of sentence 
df['num_sentences'] = df['OriginalTweet'].apply(lambda x:len(nltk.sent_tokenize(x)))

df.sample(5)

# now the point is to see relation between new created features
df[['num_characters','num_words','num_sentences']].describe()

df[df['Sentiment'] == 0][['num_characters','num_words','num_sentences']].describe()

df[df['Sentiment'] == 1][['num_characters','num_words','num_sentences']].describe()

df[df['Sentiment'] == 2][['num_characters','num_words','num_sentences']].describe()

df[df['Sentiment'] == 3][['num_characters','num_words','num_sentences']].describe()

df[df['Sentiment'] == 4][['num_characters','num_words','num_sentences']].describe()

import seaborn as sns

plt.figure(figsize=(12,6))
sns.histplot(df[df['Sentiment'] == 0]['num_characters'] , color='black')
sns.histplot(df[df['Sentiment'] == 1]['num_characters'],color='red')
sns.histplot(df[df['Sentiment'] == 2]['num_characters'],color='yellow')
sns.histplot(df[df['Sentiment'] == 3]['num_characters'],color='blue')
sns.histplot(df[df['Sentiment'] == 4]['num_characters'],color='green')

plt.figure(figsize=(12,6))
sns.histplot(df[df['Sentiment'] == 0]['num_words'])
sns.histplot(df[df['Sentiment'] == 1]['num_words'],color='red')
sns.histplot(df[df['Sentiment'] == 2]['num_words'],color='yellow')
sns.histplot(df[df['Sentiment'] == 3]['num_words'],color='blue')
sns.histplot(df[df['Sentiment'] == 4]['num_words'],color='green')

plt.figure(figsize=(12,6))
sns.histplot(df[df['Sentiment'] == 0]['num_sentences'])
sns.histplot(df[df['Sentiment'] == 1]['num_sentences'],color='red')
sns.histplot(df[df['Sentiment'] == 2]['num_sentences'],color='yellow')
sns.histplot(df[df['Sentiment'] == 3]['num_sentences'],color='blue')
sns.histplot(df[df['Sentiment'] == 4]['num_sentences'],color='green')

sns.pairplot(df,hue='Sentiment')

sns.pairplot(df,hue='Sentiment')

#3.Data Preprocessing
  # Lower case
  # Tokenization
  # Removing special characters
  # Removing stop words and punctuation
  # Stemming

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
from nltk.corpus import stopwords
nltk.download('stopwords')
import string
import re

# Function for removing HTML , URLS , Punctuations
import re
import string
def remove_html_tags(text):
    pattern = re.compile('<.*?>')
    return pattern.sub(r'', text)
def remove_url(text):
    pattern = re.compile(r'https?://\S+|www\.\S+')
    return pattern.sub(r'', text)
def remove_punc(text):
    exclude = string.punctuation
    for char in exclude:
        text = text.replace(char,'')
    return text

def transform_text(text):
    text = text.lower()
    text = nltk.word_tokenize(text)
    
    
    y = []
    for i in text:
        if i.isalnum(): #sice i dont want any special char in my data
            y.append(i)
    
    text = y[:]
    y.clear()
    
    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)
            
    text = y[:]
    y.clear()
    
    for i in text:
        y.append(ps.stem(i))
    
            
    return " ".join(y)

df['transformed_text'] = df['OriginalTweet'].apply(remove_html_tags) #it step will take time

df['transformed_text'] = df['transformed_text'].apply(remove_url) #it step will take time

df['transformed_text'] = df['transformed_text'].apply(transform_text) #it step will take time

#Word Cloud
from wordcloud import WordCloud
wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')

target1 = wc.generate(df[df['Sentiment'] == 1]['transformed_text'].str.cat(sep=" "))

target0 = wc.generate(df[df['Sentiment'] == 0]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(15,6))
plt.imshow(target1) #plt.imshow(target0)

df.sample(5)

#some exrta work for presenting
Expositive_corpus = []
for msg in df[df['Sentiment'] == 1]['transformed_text'].tolist():
    for word in msg.split():
        Expositive_corpus.append(word)

len(Expositive_corpus)

from collections import Counter
sns.barplot(pd.DataFrame(Counter(Expositive_corpus).most_common(30))[0],pd.DataFrame(Counter(Expositive_corpus).most_common(30))[1])
plt.xticks(rotation='vertical')
plt.show()

Exnegtive_corpus = []
for msg in df[df['Sentiment'] == 0]['transformed_text'].tolist():
    for word in msg.split():
        Exnegtive_corpus.append(word)
#len(ham_corpus)
from collections import Counter
sns.barplot(pd.DataFrame(Counter(Exnegtive_corpus).most_common(30))[0],pd.DataFrame(Counter(Exnegtive_corpus).most_common(30))[1])
plt.xticks(rotation='vertical')
plt.show()

# Text Vectorization
# using Bag of Words
df.head()



#Modling

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv = CountVectorizer() #use to apply Bag of words
tfidf = TfidfVectorizer(max_features=3000) #TF-IDF

X_cv = tfidf.fit_transform(df['transformed_text']).toarray()

X_tfidf = tfidf.fit_transform(df['transformed_text']).toarray()

#apply scaling 
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_tfidf = scaler.fit_transform(X_tfidf)
X_cv = scaler.fit_transform(X_cv)

# for text vectorization we can use "Word2Vec"
import gensim
import gensim.downloader as gensim_api

#load model-- it will download the model
#gen = gensim_api.load("word2vec-google-news-300")

## fit w2v
gen = gensim.models.word2vec.Word2Vec(df['transformed_text'], size=300,   
            window=8, min_count=1, sg=1, iter=30)

print(X_cv.shape)
print(X_tfidf.shape)
#print (gen.shape)

y = df['Sentiment'].values

y

"""Model Tranning"""

#Using TF-IDF , X_tfidf

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X_tfidf,y,test_size=0.2,random_state=2)
X_traincv,X_testcv,y_traincv,y_testcv = train_test_split(X_tfidf,y,test_size=0.2,random_state=2)
from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()
rf = RandomForestClassifier()

#model 1
gnb.fit(X_train,y_train)
y_pred1 = gnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
#print(precision_score(y_test,y_pred1 , average = "weighted"))
#model 2
mnb.fit(X_train,y_train)
y_pred2 = mnb.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
#print(precision_score(y_test,y_pred2))
#model 3
bnb.fit(X_train,y_train)
y_pred3 = bnb.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
#print(precision_score(y_test,y_pred3))
#model 4
rf.fit(X_train,y_train)
y_pred3 = rf.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))

#model 1
gnb.fit(X_traincv,y_traincv)
y_pred1 = gnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
#print(precision_score(y_test,y_pred1 , average = "weighted"))
#model 2
mnb.fit(X_traincv,y_traincv)
y_pred2 = mnb.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
#print(precision_score(y_test,y_pred2))
#model 3
bnb.fit(X_traincv,y_traincv)
y_pred3 = bnb.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
#print(precision_score(y_test,y_pred3))
#model 4
rf.fit(X_traincv,y_traincv)
y_pred3 = rf.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))

# Voting Classifier
mnb = MultinomialNB()
from sklearn.ensemble import VotingClassifier
voting = VotingClassifier(estimators=[('bn', bnb), ('mnb', mnb), ('rf', rf)],voting='soft')
voting.fit(X_train,y_train)
VotingClassifier(estimators=[('bn',BernoulliNB()),('mnb', MultinomialNB()),('rf',RandomForestClassifier())],voting='soft')
y_pred = voting.predict(X_test)
print("Accuracy",accuracy_score(y_test,y_pred))
#print("Precision",precision_score(y_test,y_pred))

# Applying stacking
estimators=[('bn', bnb), ('mnb', mnb), ('rt', rf)]
final_estimator=RandomForestClassifier()
from sklearn.ensemble import StackingClassifier
clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy",accuracy_score(y_test,y_pred))
#print("Precision",precision_score(y_test,y_pred))

import pickle
pickle.dump(tfidf,open('vectorizer.pkl','wb'))
pickle.dump(mnb,open('model.pkl','wb'))

